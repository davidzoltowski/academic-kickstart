---

title: "Identifiability of linear dynamical systems"
date: 2020-11
draft: true
featured: true
markup: mmark

---

Linear dynamical systems (LDS) and extensions are commonly used to model neural dynamics. This post is about the lack of identifiability of the latent variables in a linear dynamical system, with a derivation showing that similarity transformations of the latents preserve the model. 

We start with the generative model of the LDS. In an LDS, latent variables $x \in \mathbb{R}^D$ evolve with linear-Gaussian dynamics and we observe Gaussian observations $y \in \mathbb{R}^N$:
$$
\begin{aligned}
x_t &= A x_{t-1} + b + \epsilon_t, \quad \epsilon_t \sim \mathcal{N}(0, Q) \\
y_t &= C x_t + d + \eta_t, \quad \eta_t \sim \mathcal{N}(0, R).
\end{aligned}
$$
It is also possible to include a dependence of the dynamics on inputs but we do not consider that for now. 

The marginal likelihood of the data is computed by marginalizing the latent variables $x$ over all time
$$
p(y) = \int p(y, x) \mathrm{d}x.
$$
The forward-backwards algorithm is one method to perform the marginalization over $x$. 

## Invariance to invertible transformations of latents

The claim is that the marginal likelihood $\log p(y)$ is invariant to invertible transformations of the latent variables. This transformation has the form $\hat{x} = S^{-1} x$ for an invertible, square matrix $S$.  

<!--A similarity transformation is one of the form $\hat{x} = S^{-1} x S$ test  --> 

I'll first derive the log likelihood $\log p(y)$ and then show that the log likelihood is preserved with an invertible transformation of the latents (and a corresponding similarity transformation of the parameters). 

We can derive the distribution of the marginal observations $y_t$ by thinking recursively about the generative process. Let the initial value $x_0 \sim \mathcal{N}(\mu_0, Q_0)$. Then the distribution of the first observation is 
$$
y_0 = Cx_0 + d + \eta_0 \sim \mathcal{N}(C \mu_0 + d, R + C Q_0 C^\top).
$$
For $t > 0$, we work backwards to describe the marginal distribution of $y_t$
$$\begin{aligned}
y_t &= C x_t + d + \eta_t \\
& = C (A x_{t-1} + b + \epsilon_t ) + d + \eta_t \\
& = C A x_{t-1} + C b + C \epsilon_t + d + \eta_t \\
& = C A (A x_{t-2} + b + \epsilon_{t-1}) + C b + C \epsilon_t + d + \eta_t \\
& = C A^2 x_{t-2} + C A b + Cb + C A \epsilon_{t-1} + C \epsilon_t + d + \eta_t
\end{aligned}$$ Noticing a pattern, we have
$$
y_t = C A^t x_0 + \bigg( \sum_{i=0}^{t-1} C A^{i} \epsilon_{t-i} + C A^i b \bigg) + d + \eta_t
$$ such that
$$
y_t \sim \mathcal{N}\bigg( C A^t \mu_0 + d + \sum_{i=0}^{t-1}  C A^i b, R + Q_0 + \sum_{i=0}^{t-1} (C A^i) Q {(C A^i)}^\top \bigg).
$$

**Impulse response function??**

Next -> show preservation under similarity transformation. Here you need to recall the transformation of x, then redefine the parameters, then show that you get equivalent distributions for $y_t$ and $y_0$. 

$$
\begin{aligned}
y_{2} &=C x_{2}+\delta_{2} \\
&=C A x_{1}+C \eta_{1}+\delta_{2} \\
&=C A\left(A x_{0}+\eta_{0}\right)+C \eta_{1}+\delta_{2} \\
&=C A^{2} x_{0}+C A \eta_{0}+C \eta_{1}+\delta_{2} \\
y_{t} &=C A^{t} x_{0}+\sum_{i=0}^{t-1} C A^{i} \eta_{t-1-i}+\delta_{t} \\
y_{t} & \sim \mathcal{N}\left(C A^{t} x_{0}, R+\sum_{i=0}^{t-1} C A^{i} Q A^{i^{\top}} C^{\top}\right)
\end{aligned}
$$

where 

$$
\begin{aligned}
C A^{i} Q A^{i^{\top}} C^{\top} &=C S^{-1} S A^{i} S^{-1} S Q\left(S^{-1} S A^{i} S^{-1} S\right)^{\top} C^{\top} \\
&=\hat{C} \hat{A}^{i} \hat{Q}\left(\hat{A}^{i}\right)^{\top} C^{\top} \\
\hat{C} &=C S^{-1}, \hat{A}=S A S^{-1}, \hat{Q}=S Q S^{\top}
\end{aligned}
$$

and

$$
\begin{array}{l}
A=V \Sigma V^{-1} \\
\hat{A}^{i}=\left(S A S^{-1}\right)^{i}=\left(S V \Sigma V^{-1} S^{-1}\right)^{i}=S V \Sigma^{i} V^{-1} S^{-1}=S A^{i} S^{-1}
\end{array}
$$

## What about for non-Gaussian observations?

Here, derive with GLM observation. 
